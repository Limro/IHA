\documentclass[Main]{subfiles}

\begin{document}

\section{Chapter 1}

\subsection{A DMS produces symbols with the probabilities as given}

\paragraph{(a)  Find the self-information associated with each symbol, and the entropy of
the source}
Find information, $I$, in bits:

\begin{align}
I_A &= log_2\bigg(\dfrac{1}{0.4}\bigg) = 1.3219 \\
I_B = I_C &= log_2\bigg(\dfrac{1}{0.2}\bigg) = 2.3219\\
I_D &= log_2\bigg(\dfrac{1}{0.1}\bigg) = 3.3219\\
I_E = I_F &= log_2\bigg(\dfrac{1}{0.05}\bigg) = 4.3219\\
\end{align}

The entropy, $H(x)$, in bits pr. symbol:
\begin{align}
H(x) &= \sum_{i = A}^F P_i \cdot I_i \\
	&= 0.4\cdot 1.32 + 2\cdot (0.2 \cdot 2.2319) + 0.1 \cdot 3.3219 + 2 \cdot(0.05\cdot 4.3219) \\
	&= 2.2219 \dfrac{bits}{symbol}
\end{align}

\paragraph{(b) Calculate the maximum possible source entropy, and hence determine the
source efficiency}

The maximum possible source entropy:
\begin{align}
H_{max}(x) &= \dfrac{1}{M} \sum log_2 M\\
	&= log_2 (M)\\
	&= 2.58 \dfrac{bits}{symbol}
\end{align}

The the source efficiency:
\begin{align}
eff &= \dfrac{H(x)}{H_{max}(x)} \\
	&= \dfrac{2.22}{2.58} \cdot 100\% \\
	&= 86\%
\end{align}	





\subsection{}
\paragraph{(a) Calculate the entropy of a DMS that generates five symbols \{A, B, C, D, E\}
with probabilities $P_A = 1/2$, $P_B = 1/4$, $P_C = 1/8$, $P_D = 1/16$ and $P_E = 1/16$.}

The entropy, $H(x)$, in bits pr. symbol:
\begin{align}
P_i \cdot I_i &= P_i \cdot log_2\bigg(\dfrac{1}{P_i}\bigg)\\
H(x) &= \sum_{i=A}^E P_i \cdot I_i\\
	&= \dfrac{1}{2}log_2\bigg(\dfrac{1}{0.5}\bigg) + \dfrac{1}{4} log_2\bigg( \dfrac{1}{0.25} \bigg) + \dfrac{1}{8} log_2 \bigg( \dfrac{1}{0.125} \bigg) + 2\cdot \Bigg( \dfrac{1}{16} log_2 \bigg( \dfrac{1}{\frac{1}{16}} \bigg) \Bigg)\\
	&= 1.875 \dfrac{bits}{symbol}
\end{align}



\paragraph{(b) Determine the information contained in the emitted sequence DADED.}
Information in bits:
\begin{align}
I_{seq} &= I_A + 3\cdot I_D + I_E\\
	&= log_2(2) + 3\cdot log_2(16) + log_2(16)\\
	&= 17 bits
\end{align}

\subsection{}
\paragraph{Calculate the source entropy, the transinformation $I(X, Y)$ and the capacity of
the BSC define}

Source entropy, $H(x)$, in bits pr. symbol:
\begin{align}
H(x) &= P(0) log_2\bigg(\dfrac{1}{P(0)}\bigg) + P(1) log_2\bigg(\dfrac{1}{P(1)}\bigg)\\
	&= 0.2 log_2\bigg(\dfrac{1}{0.2}\bigg) + 0.8 log_2\bigg(\dfrac{1}{0.8}\bigg)\\
	&= 0.7219 \dfrac{bits}{symbol}
\end{align}

Transinformation and channelcapacity
\begin{align}
H(y) &= \sum_j P(y_i)log_2 \dfrac{1}{P(y_j)}\\
	&= 0.9341\\
H(y,x) &= \sum_{i,j} P(x_i, y_i)log_2\dfrac{1}{P(y_j/x_i)}\\
I(x,y) &= H(y) - H(y,x)\\
	&= 0.1228 \dfrac{bits}{symbol}\\	
ChannelCapacity &= 1- H(y/x)\\
	&= 0.1887
\end{align}


\begin{lstlisting}[caption=Transinformation, style=Code-Matlab, label=lst:trans]
A = [0.2 ; 0.8];
B = [0.75 0.25 ; 0.25 0.75];

%Entropy, H(X)
Hx = 0;
for i = 1:size(A,1)
    Hx = Hx + A(i,1) * log2(1 / A(i,1));
end
Hx

%H(Y/X)
HYX = Equivocation(A, B);

%H(Y)
HY = Hy(A, B);

%Mutal information, I(X/Y)
IXY = HY - HYX

%capacity of the BSC
Cs = 1 - HYX
\end{lstlisting}

The function \texttt{Equivocation} is implemented as in \codeTitle \ref{lst:equ}

\begin{lstlisting}[caption=Equivocation, style=Code-Matlab, label=lst:equ]
function res = Equivocation(A, B)
%Equivocation, H(X/Y)
%A = P(X) (all values)
%B = Channel describtion
%Sum(P(x_i, y_j)*log2(1/P(x_i/y_J)))

summa = 0;
for x = 0:length(B(:,1))-1
    for y = 0:length(B(1,:))-1
        summa = summa + Joint(A, B, x, y) * log2(1 / B(x+1,y+1));
    end
end

res = summa;
end
\end{lstlisting}

The function \texttt{Joint} is implemented as in \codeTitle \ref{lst:joi}

\begin{lstlisting}[caption=Joint, style=Code-Matlab, label=lst:joi]
function res = Joint(A, B, x, y)
% P(x_i, y_j) = P(x_i/y_j) * P(y_j)
% Joint probability

res = Backward(A, B, x, y) * Py(A, B, y);

end
\end{lstlisting}

The function \texttt{Backward} is implemented as in \codeTitle \ref{lst:back}

\begin{lstlisting}[caption=Backward, style=Code-Matlab, label=lst:back]
function res = Backward(A, B, x, y)
%Backward probability: P(x_i/y_j)
%A = P(X) (all values)
%B = Channel describtion
%x = Which x to be transmitted
%y = Which y is the condition
% The probability that symbol x_i has been transmitted if symbol y_j is 
% received, is also referred to backward probability, or a posteriori 
% probability

TotalSum = 0;
for i = 1:length(B(1,:))
    TotalSum = TotalSum + sum(B(1:end,i) .* A(1:end));
end

TotalSum = roundn(TotalSum,-2); %round down

%if PY ~= 1
if TotalSum ~= 1
    %disp('First matrixs sum != 1')
    disp('!Please check the matrix input!')
    error('Error in sum of output!')
end

res = B(x+1,y+1)*A(x+1)/Py(A, B, y);
end
\end{lstlisting}


The function \texttt{Py} is implemented as in \codeTitle \ref{lst:py}

\begin{lstlisting}[caption=Py, style=Code-Matlab, label=lst:py]
function res = Py(A, B, y)
%A = P(X) (all values)
%B = Channel describtion
%y = Which y to be calculated
%P(Y) = P_11 * P(x_1) + P_21 * P(x_2) +...+ P_u1 * P(x_u)
% Probability that a given sumbol is present at the channel ouput

    value = 0;
    for i = 1:length(A(:))
        value = value + B(i, y+1)*A(i);
    end
    
    res = value;
end
\end{lstlisting}


The function \texttt{Hy} is implemented as in \codeTitle \ref{lst:Hy}


\begin{lstlisting}[caption=Hy, style=Code-Matlab, label=lst:Hy]
function res = Hy(A, B)
% Output entropy, H(Y)
% H(Y) = P(y_1)*log2(1/P(y_1) + P(y_n)*log2(1/P(y_n)
%A = P(X) (all values)
%B = Channel describtion

val = 0;
for i = 0:length(B(1,:))-1
    val = val + Py(A, B, i) * log2(1/Py(A, B, i));
end
res = val;
end
\end{lstlisting}











\end{document}