\documentclass[danish, 10pt]{Memoir}

\include{Preamble}

\begin{document}
\chapter{OPTI lessons}

\section{Lesson 1} % (fold)
\label{sec:lesson_1}

\subsection{Matrix Games}

\begin{itemize}
	\item Opstil tabel med, hvad R får ud af alle outcomes
	\item Opskriv worst case for hver række
	\item[] Find max at disse. $max[min[a_{ij}]]$
	\item Opskriv worst vase for hver kolonne
	\item[] Find max at disse.  $min[max[a_{ij}]]$
\end{itemize}

\subsection{Proberbility vector}

\begin{align}
x &= [x_1, x_2 \ldots, x_m] \\
\sum x_i &= 1\\
y &= [y_1, y_2 \ldots, y_n] \\
\sum y_i &= 1\\
E[x,y] &= \sum_{j=1}^n \sum_{i=1}^m x_i \cdot a_{ij} \cdot y_i \\
	&= x^T \cdot A \cdot y
\end{align}


% section lesson_1 (end)


\section{Lesson 2}

Linear Programming -- Geometric method and Simplex method.
\\
Slide 3
Maximer profit.
\begin{itemize}
\item Opskriv max. profit med $f(x_1, x_2) = Ax_1 + Bx_2$
\item Opskriv formlerne med $a \cdot x_1 + b \cdot x_2 \leq y_1$
\item Gør det muligt med $x_1 \geq 0, x_2 \geq 0$
\item Hvis der er flere "udgange" for et punkt, skrives $x_{input1}+x_{input2} = y_{output1}+y_{output2}$
\item Udregn
\end{itemize}
Slide 5 -- Definition
\begin{itemize}
\item Ved udregning af $Ax \geq b$ udregnes A's række sum (hver række, $m$'s, sum) og sammenlignes med $b_m$
\end{itemize}
Slide 7 -- theorem
\begin{itemize}
\item Du skal lede ved hjørnerne!
\end{itemize}
Slide 8 -- eksempel
\begin{itemize}
\item Følg slide 3
\item Tegn en graf med variablerne $x_1$ og $x_2$
\item Akserne har max-værdier (conditions)
\item Ingår flere variabler i én formel, så isoler det der er på y-aksen ($x_2$)
\item Kaldes et "konvex set"
\item Sæt extream points in i en tabel og udregn værdierne, og find max
\item I stedet for extream points kan man isolere $x_2$ i "max.-formlen" og finde den værdi, der kun rører i ét punkt (men det er lort!).
\end{itemize}
Slide 9 -- Slack variabler
\begin{itemize}
\item Tilføj slack variabler for at gøre $\leq$ til $=$
\item Basisløsningen er, at de nye basisværdier er det, de er lig med. Således er $x_1, x_2 \ldots = 0$
\end{itemize}
Slide 10 -- Elementary operators on liniar equation systems (pivot)
\begin{itemize}
\item Take one row, multiply with a number and add to another row
\item Interchange to row
\item Multiply 1 row with a number $\not = 0$
\item Opskriv matrix med $x_1, x_2 \ldots y$
\end{itemize}



\newpage
\section*{Lesson 3 -- duality}

\begin{itemize}
	\item Man har maximize og minimize
	\item Primal betegnes med (P)
	\item Duality betegnes med (D) eller (P')
	\item Noter, at $A^T y \geq c$
	\item Byt $c$ og $b$ og transpose $A$
\end{itemize}






\newpage
\section*{Lesson 4 -- The general problem}

\begin{itemize}
	\item Der arbejdes i planer
	\item \texttt{Local minimizer} betyder, at det er det laveste punkt for et lille område på en parabel / funktion
	\item \texttt{Global minimizer} er den mindste af dem alle
	\item \texttt{Optimal solution} og \texttt{minimizer} kan være det samme
	\item $(D^2 f)^T = D^2 f$ er symmetrisk
\end{itemize}
Directional derivatives

\begin{itemize}
	\item Er i 3D
	\item Hvis du har et punkt på en overflade og det bevæges i en bestemt retning - hvad sker der så med værdien på overfalden?
	\item $f(x+\alpha d)$
	\item Chain rule: $\dfrac{d}{dx} f(y(x)) = \dfrac{df}{dy} \cdot \dfrac{dy}{dx}$
\end{itemize}
Feasible direction

\begin{itemize}
	\item Blot en mulig retning hvor $x \in \Omega$
\end{itemize}
Quadratisk form -- med matrix

\begin{itemize}
	\item Skrives som $Q(x_1 x_2) = x_1^2 + x_2^2 + 3x_1 x_2$
	\item $Q : \mathbb{R}^2 \rightarrow \mathbb{R}$
	\item $Q(x) = x^T A x$
\end{itemize}

$Q(x) = $
\begin{ArgMat}
x_1 & x_2
\end{ArgMat}
\begin{ArgMat}
a & b \\ c & d
\end{ArgMat}
\begin{ArgMat}
x_1 \\ x_2
\end{ArgMat}




\newpage
\section{Lesson 5 -- One dimentional problem}

Der bruges forskellige metoder til at finde \texttt{unimodal funktionen}.

\subsection{Golden Section Search}

\begin{itemize}
	\item Indsæt 2 punkter: $a_0, b_0$
	\item Indsæt 2 nye punkter, $a_1$ i afstanden $a_1 = a_0+ \varrho \cdot(b_0-a_0)$ og $b_1$ med konstant afstand fra $a_0$ til $a_1$ og samme afstand fra $b_0$ til $b_1$ i afstanden $b_1 = b_0 - \varrho \cdot(b_0-a_0)$
	\item Hvis afstanden $f(a_1) < f(b_1) \Rightarrow [a_0, b_1]$
	\item Hvis afstanden $f(a_1) > f(b_1) \Rightarrow [a_1, b_0]$
	\item Kør rekursivt
	\item Insæt nye punkter $a_n$ og $b_n$
	\item Forsøg herefter at få $f(a_n) < f(b_n)$ hvor $f(b_n) = f(b_{n-1}) \Rightarrow [a_0,b_n]$
	\item Forsøg at få $f(a_n) > f(b_n)$ hvor $f(b_n) = f(b_{n-1}) \Rightarrow [a_2,b_0]$
	\item Når $a_{n-1} = b_n$ gælder den gyldne regel: $\dfrac{3- \sqrt{5}}{2} 
	= 0.382$

\end{itemize}


\subsection{Fibonacci Search 1D}
Denne metode er bedre, da intervallet bliver større (og går derfor hurtigere).

\begin{align*}
1-2 \varrho_k &= \varrho_{k+1}(1-\varrho_k) \\
\varrho_{k+1} &= \dfrac{1-2 \varrho_k}{1-\varrho_k}
\end{align*}





\subsection{Newton's Method 1D}
Denne er endnu bedre.

$f(x) = f(x_0) + \dfrac{f'(x_0)}{1!}(x-x_0) + \dfrac{f'(x_0)}{2!}(x-x_0)^2 + \mathbb{R}(x-x_0) $

\begin{itemize}
	\item Find $a$ min. for $f$, brug Tayler exp.
	\item $f(x) \sim{=} q(x)$
	\item $q'(x)=0 $
	\item $q'(x) = \dfrac{f'(x_0)}{1!} + \dfrac{f'(x_0)}{2!}\cdot 2(x-x_0) + \mathbb{R}(x-x_0)$
	\item $X_{k+1} = X_k-\dfrac{f'(x_k)}{f''(x_k)}$ and set $g(x) = f'(x)$
\end{itemize}


\subsection{Secant method 1D}





\newpage
\section*{Lesson 6 - Gradient methods}

\subsection{descent formula}
\begin{itemize}
	\item Gradient er en column vector som skal afledes $\nabla f'(x)$
	\item En kurve i et plan, hvor der kan tegnes en vector retning. Det er hældningen i ét punkt.
	\item Retningen er den vej, man går mest op ad kurven.
	\item Hvis man vil gå til et nyt sted $x^{k+1}$ fra et startsted, $x^k$, kan man flytte sig ved at trække $x \cdot \alpha \nabla f |_{x=x^k}$ fra, altså $x^{k+1} = x^k - x \cdot \alpha \nabla f |_{x=x^k}$
	\item Hvis alpha er for stor tager man for store spring og vil derfor aldrig finde minimum.
\end{itemize}

\subsection{Steepest descent algorithm}
Hvordan finder man det bedste punkt?
Kan bruges til at fjerne støj

\begin{itemize}
	\item $\alpha_k = \underset{\alpha \geq 0}{\text{arg min}} = f(x^{k+1} = x^k - x \cdot \alpha \nabla f |_{x=x^k})$
	\item Når $\nabla f(x^{k+1}) = 0$ stopper man
	\subitem Se slides for flere stop-kriterier.
\end{itemize}


\subsection{Quadratisk form}

$\alpha _k = \underset{\alpha}{\text{arg}} \{ f(x^k - \alpha \nabla f|_{x=x^k}) \}$ 
\\
$f(x) = \frac{1}{2} x^T Q x - b^T x$ \quad $Q^T = Q$
\\
$\nabla f = Q x - b$



\newpage
\section*{Lesson 7}

\subsection*{Newton's Methods $n$D}
\begin{itemize}
	\item Brug Taylor's metode og afled det antal gange der skal bruges
	\item Se slides for formel
\end{itemize}


\subsection*{Data fitting}
\begin{itemize}
 	\item Data kan bruges til, at lave en bedre kurve
 	\item Det kan ex. være en hjerterytme.
 \end{itemize} 




\newpage
\section*{Lesson 8}	

\subsection*{Conjugate Direction methods}

I stedet for at zig-zagge kan man gå mere direkte igennem nogle cirker.

\begin{itemize}
	\item $f(X) = \dfrac{1}{2} x^T Q-x^T$
	\item $d^{(0)}, d^{(1)} \ldots d^{(k)}$ er conjugeret mht $Q$ hvis ${d^{(j)}}^T Q d^{(i)}, j \not = i$
	\item Eigen værdierne skal være positive.
\end{itemize}






\newpage
\section*{Lesson 9}

\begin{itemize}
	\item Når man bruger linear equations kan de løses med slack variabler
	\item Men hvis der sker afrundinger kan der opstå fejl
	\item Dette løses med \texttt{relative residual}
	\item[] $\dfrac{||b - Ax'||}{||b||} = \dfrac{||r||}{||b||}$
	\item Derudover er der relative error, som \dots
	\item[] $\dfrac{||x' - x||}{||x||} = \dfrac{||e||}{||b||}$
	\item Denne værdi skal estimeres, da $x$ er ukendt:
	\item[] $r = b-Ax'$ \dots
	\item[] $r = A \cdot e$
	\item[] $||A \cdot e|| = ||r|| \Leftrightarrow ||A|| \cdot ||e \geq ||r||$
	\item[] $||A^{-1} r|| = ||e|| \Leftrightarrow ||A^{-1}|| \cdot ||e \geq ||r||$
	\item[] $||A|| \cdot ||x|| \geq ||b||$
	\item[] $||A^{-1}|| \cdot ||b|| \geq ||x||$
	\item Dette kan oversættes til $\dfrac{1}{||a|| \cdot ||A^{-1}|| \cdot ||b||} \dfrac{||r||}{||b||} \leq \dfrac{||e||}{||x||} \leq ||A|| \cdot ||A^{-1}|| \cdot \dfrac{||r||}{b||}$
	\item[] 
	$||$\begin{ArgMat}
	a_1 \\ \dots \\ a_n
	\end{ArgMat}$||_\infty = \underset{i}{max}\text{ }|a_i|$
\end{itemize}

Eksempel:
\begin{itemize}
	\item Hvis man har $Ax=b$ er der ikke nødvendigvis en løsning.
	\item Omskriv til $||Ax-b||$ og find minimum, som en funktion af $x$.
	\item $Ax = proj_{R(A)}^b = A \cdot	(AA^T)^{-1} \cdot A^T b$
\end{itemize}

Eksempel 2:
\begin{itemize}
	\item For $f(x) = ||Ax-b||^2$ er $\nabla f = A^TAx - A^Tn $
	\item[] $\Rightarrow A^TAx-A^Tb = 0$
	\item[] $\Leftrightarrow x = (A^TA)^{-1}A^T b$
\end{itemize}


\newpage
\section*{Genetic algorithm}
\begin{itemize}
	\item Bruges i mange tilfælde
	\item Kommer fra genetik
\end{itemize}
Virker som følger:
\begin{itemize}
	\item Map $\Omega$ ind i $\{0, 1\}^L$ (Power set der er $L$ lang)
	\item Ét $L$ kaldes et kromosom.
	\item Fitness (det der passer bedst: $f: \{0,1\}^L \rightarrow R$)
	\item Definer population med $k$ iterationer: $P(k)$
	\item Definer mating pool: $M(k)$
	\item Start med $P(0)$, lav en selection og få $M(0)$, evolution giver $P(1)$ \dots
	\item Select de kromosomer, der har en høj fitness
	\subitem Selection kaldes også "fitness sampling"
	
	\item I evolution laver mancrossover:
	De 2 der crosses kaldes "parents", mens outcome kaldes for "offspring kromosomer"
	\item Man laver et crossover point et sted (behøver ikke være specificeret\dots)
\end{itemize}







\newpage
\section*{Problems with equality constraints}

Der er tre modeller:
\begin{itemize}
	\item Linear ($f(x_1 x_2) = $x gange x divider\dots)
	\item Non-linear ($f(x_1 x_2) = $cos/sin gange\dots)
	\item Constraints ($x_1^2 + x_2^2 = 1$)
\end{itemize}
Hvad gør man med constraints:
\begin{itemize}
	\item Tegn en 3D model.
	\item Lav en cirkel i rummet $h(x_1 x_2) = (x-x_1)^2 + (y-y_1)^2-r$.
	\item Find nogle punkter på cirklen og de tilhørende værdier på 3D-modellen.
	\item Der er gerne flere constraints
	\item For hver constrain reduceres det dimensionerne med 1, hvis $\nabla h_i(x)$ er linear independant i alle punkter (Der må ikke være x'er i resultatet).
	\item Hvis der ikke reduceres i dimensioner findes enten et plan (der rammer ét bestemt punkt med en bestemt vinkel), en linje (tangent), eller et punkt.
	\item Dette punkt kaldes tangent space og beskrives som $T(x*) = \{ y | \nabla h^T y = 0 \}$, hvor $x*$ er punktet der skal rammes.
	\item Jacobian er $Dh(x) = \begin{bmatrix}
	\nabla h_1(x)^T \\ \nabla h_2(x)^T
	\end{bmatrix}$
	\item Normal space er  $T(x*)^\perp = N(x*)$, altså det ortogonale rum af tangenten.
\end{itemize}

\subsection*{Lagrange condition}
\begin{itemize}
	\item Gælder for en cirkel med rumfang (en cylinder).
	\item Find gradienten af $f$ og af $h$: $\nabla f(x*) + \lambda \nabla h(x*) = 0$, hvilket giver ukendt $\lambda$.
	\item Hvis man $\dfrac{d}{dt}|_{t=0} f(x(t)) = 0$ pga FONC reglen
	\item Når man påfører en elipse på et bøjet plan får man en elipse der ikke følger $x$-planet.
	\item Ved at indsætte tal i ovenståedn formel fåes 3 ukendte i 2 ligninger
	\item Dette løses ved at indsætte elipsen i matrixen.
	 (3 ligninger med 3 ubekendte).
	 \item Forsøg herefter af isolere ALLE $x$'erne og opstil de resterende linger, så de bliver 0. 
	 Dette giver normal nogle punkter for hver $x_i = 0$
	 \item Herefter kan tallene indsættes i elipse-formlen, $h(x_i x_j x_k)$, og se hvor minimizeren er.
\end{itemize}
Eksempel xx
\begin{itemize}
	\item Definer en kasse
	\item Definer volume
	\item Lav inddelinger
	\item Opskriv $f(x y x)$ med flader af kombinationerne $xy + xz + yz$
	\item Opskriv $f(x y z) = xyz - volume$
	\item Find $\nabla$ af $f$ og $h$
	\item Indsæt i matrix
	\item Find muligheder for hver row = 0
\end{itemize}
Maximer
\begin{itemize}
	\item 
\end{itemize}









\newpage
\section*{Opsumering}

\subsection{Optimization} % (fold)
\label{sub:calculus}
\begin{itemize}
	\item Der findes lokale og globale minimizers
	\item Disse skrives som $x*$
	\item En lokal minimizer kan også findes på en flad linje, da $f(x) \geq f(x*)$
	\item Gradienten er $\nabla f(x)$ (column vector)
	\item Første afledte er en row vector, $Df(X) = \nabla f(x)^T$
	\item Anden orden er $D^2f(x)$, hvilket giver flere rækker
	\item Når man finder anden orden kaldes det "kvadratisk form".
	\item[] Denne form findes også med $\frac{1}{2}x^T Qx - x^T b$
	\item Retningen kan findes ved at indsætte startpunktet, $x_0$, i $Qx-b$
	\item \textbf{Chain rule!}
	\item \textbf{Firs-Order Necessary Condition} (FONC)
	\item[] En minimizer er har en positiv hældning (Da ligemeget hvor du går hen går du opad og du står i minimum). Dette er $\nabla f(x*)=0$
	\item Hvis man kan finde et $d$, således $d^T(\nabla f)_ {x=x_0}$ bliver negativ, så er man \emph{ikke} i FONC.
	\item $Q(x) = x^T A x$
	\item \textbf{Second-Order Necessary COndition} (SONC) - her er $F$ Hessian af $f$
	\item Fortæl ikke om feasible direction

\end{itemize}
% subsection calculus (end)


\subsection{Gradient} % (fold)
\label{sub:gradient}

\begin{itemize}
	\item Man tager Taylor epansion i ét punkt
	\item Det er en førsteordens tilnærmelse
	\item 1D: $f(x) = f(x_0) + \dfrac{1}{!} f'(x_0)(x-x_0)$
	\item Det kan tilnærmes via en ret linje
	\item nD: $f(x) = f(x_0) + \dfrac{1}{!} (\nabla f)^T(x_0)(x-x_0)$

	\item For at flytte sig skal $f(x_1) < f(x_0)$ -- altså skla resten være negativ
	\item Man flytter sig i retningen $d$
	\item Det nye punkt findes ved $x_1 = x_0 + \alpha d$, således at $\alpha ( \nabla f |_{x=x_0})^T d < 0$

	\item Det mest negative er når $d = (\nabla f |_{x=x_0})$
	\item På denne måde nærmer man sit minimum.
	\item Det kaldes også "steepest decend", når man nærmer sig.
	\item Det foregår i iterationer -- hvor meget skal man flytte sig, og i hvor stort et interval?
	\item Ved cirkler bevæger man sig til man ikke kan mere, vender sig ortogonalt og repeat (langsom metode).
	\item Man bruger et stopkriterie.
\end{itemize}

% subsection gradient (end)

\subsection{Conjugate direction} % (fold)
\label{sub:conjugate_direction}

\begin{itemize}
	\item Kræver ikke Hessian.
	\item Foregår i iterationer.
	\item Kræver ingen matrix inversion.
	\item En mellemting mellem Steepest Decent og Newton's Method.
	
	\item For $d_0, d_1 , d_2 \dots d_{n-1}, Q > 0, Q^T = Q$
	\item $d_i^T Q d_j \begin{Bmatrix}
	= 0 & i \not= j \\
	\not= 0 & i = j
	\end{Bmatrix}$
	\item Det er conjugated directions, når de er ortogonale?
	\item Vi har alle $d$'erne -- hvorfra er ligegyldigt, men vi har dem.
	\item FONC vil ikke sikre et minimum.
	\item Se slides for fremgangsmetode.
	\item Husk eksempel.
	\item Man ændrer gradienterne, så de bliver conjugated.
	\item For hver iteration finder man minimum af et space.
	\item Forklar sammenhængen mellem algoritmerne (conjugate gradient algorithm og )
\end{itemize}
% subsection conjugate_direction (end)







\end{document}