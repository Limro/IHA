\documentclass[10pt, a4]{Memoir}

\include{Preamble}

\begin{document}
\chapter{OPTI lessons}

\section{Lesson 1} % (fold)
\label{sec:lesson_1}

\subsection{Matrix Games}

\begin{itemize}
	\item Opstil tabel med, hvad R får ud af alle outcomes
	\item Opskriv worst case for hver række
	\item[] Find max at disse. $max[min[a_{ij}]]$
	\item Opskriv worst vase for hver kolonne
	\item[] Find max at disse.  $min[max[a_{ij}]]$
\end{itemize}

\subsection{Proberbility vector}

\begin{align}
x &= [x_1, x_2 \ldots, x_m] \\
\sum x_i &= 1\\
y &= [y_1, y_2 \ldots, y_n] \\
\sum y_i &= 1\\
E[x,y] &= \sum_{j=1}^n \sum_{i=1}^m x_i \cdot a_{ij} \cdot y_i \\
	&= x^T \cdot A \cdot y
\end{align}


% section lesson_1 (end)


\section{Lesson 2}

Linear Programming -- Geometric method and Simplex method.
\\
Slide 3
Maximer profit.
\begin{itemize}
\item Opskriv max. profit med $f(x_1, x_2) = Ax_1 + Bx_2$
\item Opskriv formlerne med $a \cdot x_1 + b \cdot x_2 \leq y_1$
\item Gør det muligt med $x_1 \geq 0, x_2 \geq 0$
\item Hvis der er flere "udgange" for et punkt, skrives $x_{input1}+x_{input2} = y_{output1}+y_{output2}$
\item Udregn
\end{itemize}
Slide 5 -- Definition
\begin{itemize}
\item Ved udregning af $Ax \geq b$ udregnes A's række sum (hver række, $m$'s, sum) og sammenlignes med $b_m$
\end{itemize}
Slide 7 -- theorem
\begin{itemize}
\item Du skal lede ved hjørnerne!
\end{itemize}
Slide 8 -- eksempel
\begin{itemize}
\item Følg slide 3
\item Tegn en graf med variablerne $x_1$ og $x_2$
\item Akserne har max-værdier (conditions)
\item Ingår flere variabler i én formel, så isoler det der er på y-aksen ($x_2$)
\item Kaldes et "konvex set"
\item Sæt extream points in i en tabel og udregn værdierne, og find max
\item I stedet for extream points kan man isolere $x_2$ i "max.-formlen" og finde den værdi, der kun rører i ét punkt (men det er lort!).
\end{itemize}
Slide 9 -- Slack variabler
\begin{itemize}
\item Tilføj slack variabler for at gøre $\leq$ til $=$
\item Basisløsningen er, at de nye basisværdier er det, de er lig med. Således er $x_1, x_2 \ldots = 0$
\end{itemize}
Slide 10 -- Elementary operators on liniar equation systems (pivot)
\begin{itemize}
\item Take one row, multiply with a number and add to another row
\item Interchange to row
\item Multiply 1 row with a number $\not = 0$
\item Opskriv matrix med $x_1, x_2 \ldots y$
\end{itemize}



\newpage
\section*{Lesson 3 -- duality}

\begin{itemize}
	\item Man har maximize og minimize
	\item Primal betegnes med (P)
	\item Duality betegnes med (D) eller (P')
	\item Noter, at $A^T y \geq c$
	\item Byt $c$ og $b$ og transpose $A$
\end{itemize}






\newpage
\section*{Lesson 4 -- The general problem}

\begin{itemize}
	\item Der arbejdes i planer
	\item \texttt{Local minimizer} betyder, at det er det laveste punkt for et lille område på en parabel / funktion
	\item \texttt{Global minimizer} er den mindste af dem alle
	\item \texttt{Optimal solution} og \texttt{minimizer} kan være det samme
	\item $(D^2 f)^T = D^2 f$ er symmetrisk
\end{itemize}
Directional derivatives

\begin{itemize}
	\item Er i 3D
	\item Hvis du har et punkt på en overflade og det bevæges i en bestemt retning - hvad sker der så med værdien på overfalden?
	\item $f(x+\alpha d)$
	\item Chain rule: $\dfrac{d}{dx} f(y(x)) = \dfrac{df}{dy} \cdot \dfrac{dy}{dx}$
\end{itemize}
Feasible direction

\begin{itemize}
	\item Blot en mulig retning hvor $x \in \Omega$
\end{itemize}
Quadratisk form -- med matrix

\begin{itemize}
	\item Skrives som $Q(x_1 x_2) = x_1^2 + x_2^2 + 3x_1 x_2$
	\item $Q : \mathbb{R}^2 \rightarrow \mathbb{R}$
	\item $Q(x) = x^T A x$
\end{itemize}

$Q(x) = $
\begin{ArgMat}
x_1 & x_2
\end{ArgMat}
\begin{ArgMat}
a & b \\ c & d
\end{ArgMat}
\begin{ArgMat}
x_1 \\ x_2
\end{ArgMat}




\newpage
\section{Lesson 5 -- One dimentional problem}

Der bruges forskellige metoder til at finde \texttt{unimodal funktionen}.

\subsection{Golden Section Search}

\begin{itemize}
	\item Indsæt 2 punkter: $a_0, b_0$
	\item Indsæt 2 nye punkter, $a_1$ i afstanden $a_1 = a_0+ \varrho \cdot(b_0-a_0)$ og $b_1$ med konstant afstand fra $a_0$ til $a_1$ og samme afstand fra $b_0$ til $b_1$ i afstanden $b_1 = b_0 - \varrho \cdot(b_0-a_0)$
	\item Hvis afstanden $f(a_1) < f(b_1) \Rightarrow [a_0, b_1]$
	\item Hvis afstanden $f(a_1) > f(b_1) \Rightarrow [a_1, b_0]$
	\item Kør rekursivt
	\item Insæt nye punkter $a_n$ og $b_n$
	\item Forsøg herefter at få $f(a_n) < f(b_n)$ hvor $f(b_n) = f(b_{n-1}) \Rightarrow [a_0,b_n]$
	\item Forsøg at få $f(a_n) > f(b_n)$ hvor $f(b_n) = f(b_{n-1}) \Rightarrow [a_2,b_0]$
	\item Når $a_{n-1} = b_n$ gælder den gyldne regsl: $\dfrac{3- \sqrt{5}}{2} = 0.382$

\end{itemize}


\subsection{Fibonacci Search 1D}
Denne metode er bedre, da intervallet bliver større (og går derfor hurtigere).

\begin{align*}
1-2 \varrho_k &= \varrho_{k+1}(1-\varrho_k) \\
\varrho_{k+1} &= \dfrac{1-2 \varrho_k}{1-\varrho_k}
\end{align*}

\begin{itemize}
	\item 

\end{itemize}



\subsection{Newton's Method 1D}
Denne er endnu bedre.

$f(x) = f(x_0) + \dfrac{f'(x_0)}{1!}(x-x_0) + \dfrac{f'(x_0)}{2!}(x-x_0)^2 + \mathbb{R}(x-x_0) $

\begin{itemize}
	\item Find $a$ min. for $f$, brug Tayler exp.
	\item $f(x) \sim{=} q(x)$
	\item $q'(x)=0 $
	\item $q'(x) = \dfrac{f'(x_0)}{1!} + \dfrac{f'(x_0)}{2!}\cdot 2(x-x_0) + \mathbb{R}(x-x_0)$
	\item $X_{k+1} = X_k-\dfrac{f'(x_k)}{f''(x_k)}$ and set $g(x) = f'(x)$
\end{itemize}


\subsection{Secant method 1D}





\newpage
\section*{Lesson 6 - Gradient methods}

\subsection{descent formula}
\begin{itemize}
	\item Gradient er en column vector som skal afledes $\nabla f'(x)$
	\item En kurve i et plan, hvor der kan tegnes en vector retning. Det er hældningen i ét punkt.
	\item Retningen er den vej, man går mest op ad kurven.
	\item Hvis man vil gå til et nyt sted $x^{k+1}$ fra et startsted, $x^k$, kan man flytte sig ved at trække $x \cdot \alpha \nabla f |_{x=x^k}$ fra, altså $x^{k+1} = x^k - x \cdot \alpha \nabla f |_{x=x^k}$
	\item Hvis alpha er for stor tager man for store spring og vil derfor aldrig finde minimum.
\end{itemize}

\subsection{Steepest descent algorithm}
Hvordan finder man det bedste punkt?
Kan bruges til at fjerne støj

\begin{itemize}
	\item $\alpha_k = \underset{\alpha \geq 0}{\text{arg min}} = f(x^{k+1} = x^k - x \cdot \alpha \nabla f |_{x=x^k})$
	\item Når $\nabla f(x^{k+1}) = 0$ stopper man
	\subitem Se slides for flere stop-kriterier.
\end{itemize}


\subsection{Quadratisk form}

$\alpha _k = \underset{\alpha}{\text{arg}} \{ f(x^k - \alpha \nabla f|_{x=x^k}) \}$ 
\\
$f(x) = \frac{1}{2} x^T Q x - b^T x$ \quad $Q^T = Q$
\\
$\nabla f = Q x - b$


\end{document}